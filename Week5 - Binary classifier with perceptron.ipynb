{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digit classification with  Perceptron\n",
    "\n",
    "* Perceptron can only handle binary classification. So we are going to classify the handwritten image as zero and not zero. \n",
    "* Dataset: [MNIST](https://en.wikipedia.org/wiki/MNIST_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "#skearn importts\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import hinge_loss\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# make pretty images\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data matrix X and the respective label vector y need to be converted to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's look at information like number of features, number of classes etc about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = np.unique(y)\n",
    "print('Number of samples: {0}, type: {1}'.format(X.shape[0], X.dtype))\n",
    "print('Number of features: {0}'.format(X.shape[1]))\n",
    "print('Minimum:{0}, Maximum:{1}'.format(np.min(X), np.max(X)))\n",
    "print('Number of classes: {0}, type:{1}'.format(len(target_names), y.dtype))\n",
    "print('Labels: {0}'.format(target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The feature values are pixel values of 28 x 28 pixel images, and the value ranges between 0 and 255.\n",
    "* Let's first scale the values to a [0,1]. We can use either `MinMaxScaler` or `MaxAbsScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MinMaxScaler().fit_transform(X)\n",
    "print('Minimum:{0}, Maximum:{1}'.format(np.min(X), np.max(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "Let's pick a few images and display them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 9 # Choose a square number\n",
    "factor = int(np.sqrt(num_images))\n",
    "fig, ax = plt.subplots(nrows = factor, ncols = factor, figsize = (8, 6))\n",
    "idx_offset = 0 # take \"num_images\" starting from the index \"idx_offset\"\n",
    "for i in range(factor):\n",
    "    index = idx_offset + i*(factor)\n",
    "    for j in range(factor):\n",
    "        ax[i, j].imshow(X[index + j].reshape(28, 28), cmap = 'gray')\n",
    "        ax[i, j].set_title('Label:{0}'.format(str(y[index + j])))\n",
    "        ax[i, j].set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "* Let's use a train-test split ratio of 0.6. Since the images are already randomly shuffled we need not use `train_test_spit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before proceeding further, we need to check whether the dataset is balanced or  imbalanced. We can do it by plotting the distribution of sampls in each classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.histplot(data = np.int8(y_train), binwidth = 0.45, bins = 11)\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "plt.xlabel('Class')\n",
    "plt.title('Distribution of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification: 0-Detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying Labels\n",
    "\n",
    "* We are trying to implement binary classifier which can detect 0 from the handwritten digits. \n",
    "* We will relabel the samples such that the label 0 will be chnaged to 1 and all otherr labels(1-9) will be changed to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new variable names with all -1\n",
    "y_train_0 = -1*np.ones((len(y_train)))\n",
    "y_test_0 = -1*np.ones((len(y_test)))\n",
    "\n",
    "#find indices of the digit 0 image\n",
    "indx_0 = np.where(y_train == '0')\n",
    "y_train_0[indx_0] = 1\n",
    "\n",
    "indx_0 = np.where(y_test == '0')\n",
    "y_test_0[indx_0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 9 # Choose a square number\n",
    "factor = int(np.sqrt(num_images))\n",
    "fig, ax = plt.subplots(nrows = factor, ncols = factor, figsize = (8, 6))\n",
    "idx_offset = 0 # take \"num_images\" starting from the index \"idx_offset\"\n",
    "for i in range(factor):\n",
    "    index = idx_offset + i*(factor)\n",
    "    for j in range(factor):\n",
    "        ax[i, j].imshow(X[index + j].reshape(28, 28), cmap = 'gray')\n",
    "        ax[i, j].set_title('Label:{0}'.format(str(y_train_0[index + j])))\n",
    "        ax[i, j].set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models\n",
    "\n",
    "Let's quickly construct a baseline model witht he following rule\n",
    "\n",
    "1. Count the number of samples per class\n",
    "2. The model always outputs the class which has highest number of samples.\n",
    "3. Then calculate teh accuracy of the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos = len(np.where(y_train_0 == 1)[0])\n",
    "num_neg = len(np.where(y_train_0 == -1)[0])\n",
    "print(num_pos, num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_clf = DummyClassifier(strategy='most_frequent')\n",
    "base_clf.fit(x_train, y_train_0)\n",
    "print('Training accuracy:{0:0.2f}'.format(base_clf.score(x_train, y_train_0)))\n",
    "print('Testing accuracy:{0:0.2f}'.format(base_clf.score(x_test, y_test_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "bin_clf  = Perceptron(max_iter=100, random_state=1729)\n",
    "\n",
    "#Train\n",
    "bin_clf.fit(x_train, y_train_0)\n",
    "\n",
    "#Results\n",
    "print('Dimension of weight w: {0}'.format(bin_clf.coef_.shape))\n",
    "print('Bias: {0}'.format(bin_clf.intercept_))\n",
    "print('The loss function: {0}'.format(bin_clf.loss_function_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: ', bin_clf.score(x_train, y_train_0))\n",
    "print('Test Accuracy: ', bin_clf.score(x_test, y_test_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_0 = bin_clf.predict(x_test)\n",
    "\n",
    "num_images = 9 # Choose a square number\n",
    "factor = int(np.sqrt(num_images))\n",
    "fig, ax = plt.subplots(nrows = factor, ncols = factor, figsize = (8, 6))\n",
    "idx_offset = np.random.randint(0, int(len(x_test)/2)) # take \"num_images\" starting from the index \"idx_offset\"\n",
    "for i in range(factor):\n",
    "    index = idx_offset + i*(factor)\n",
    "    for j in range(factor):\n",
    "        ax[i, j].imshow(x_test[index + j].reshape(28, 28), cmap = 'gray')\n",
    "        ax[i, j].set_title('Prediction:{0}'.format(str(y_hat_test_0[index + j])))\n",
    "        ax[i, j].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_0 = np.where(y_test_0 == 1)\n",
    "\n",
    "zeroImgs = x_test[indx_0[0]]\n",
    "zeroLabls = y_hat_test_0[indx_0[0]]\n",
    "\n",
    "num_images = 9 # Choose a square number\n",
    "factor = int(np.sqrt(num_images))\n",
    "fig, ax = plt.subplots(nrows = factor, ncols = factor, figsize = (8, 6))\n",
    "idx_offset = 0 # take \"num_images\" starting from the index \"idx_offset\"\n",
    "for i in range(factor):\n",
    "    index = idx_offset + i*(factor)\n",
    "    for j in range(factor):\n",
    "        ax[i, j].imshow(zeroImgs[index + j].reshape(28, 28), cmap = 'gray')\n",
    "        ax[i, j].set_title('Prediction:{0}'.format(str(zeroLabls[index + j])))\n",
    "        ax[i, j].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_0 = bin_clf.predict(x_train)\n",
    "cm_display = ConfusionMatrixDisplay.from_predictions(y_train_0, y_hat_train_0, values_format='.5g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = cm_display.confusion_matrix\n",
    "tn = cf_matrix[0,0]\n",
    "fn = cf_matrix[1,0]\n",
    "fp = cf_matrix[0,1]\n",
    "tp = cf_matrix[1,1]\n",
    "\n",
    "precision = tp/(tp + fp)\n",
    "recall = tp/(tp + fn)\n",
    "accuracy = (tn + tp)/ (tn + fn + fp + tp)\n",
    "\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Accuracy: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_clf = Perceptron(max_iter=100, random_state=1729)\n",
    "scores = cross_validate(bin_clf, x_train, y_train_0, cv=5, \n",
    "                        scoring=['precision', 'recall', 'f1'],\n",
    "                        return_estimator=True)\n",
    "\n",
    "pprint(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average and standard deviation of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('f1,        avg:{0:.2f}, std:{1:.3f}'.format(scores['test_f1'].mean(), scores['test_f1'].std()))\n",
    "print('precision, avg:{0:.2f}, std:{1:.2f}'.format(scores['test_precision'].mean(), scores['test_precision'].std()))\n",
    "print('f1,        avg:{0:.2f}, std:{1:.2f}'.format(scores['test_recall'].mean(), scores['test_recall'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's pick the first estimator and hope that it has better performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_clf = scores['estimator'][0]\n",
    "y_hat_test_0 = bin_clf.predict(x_test)\n",
    "cm_display = ConfusionMatrixDisplay.from_predictions(y_test_0, y_hat_test_0, values_format='.5g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision %.2f' %precision_score(y_test_0, y_hat_test_0))\n",
    "print('Recall %.2f' %recall_score(y_test_0, y_hat_test_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to generalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_0 = cross_val_predict(bin_clf, x_train, y_train_0, cv=5)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay.from_predictions(y_train_0, y_hat_train_0, values_format='.5g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = cm_display.confusion_matrix\n",
    "tn = cf_matrix[0,0]\n",
    "fn = cf_matrix[1,0]\n",
    "fp = cf_matrix[0,1]\n",
    "tp = cf_matrix[1,1]\n",
    "\n",
    "precision = tp/(tp + fp)\n",
    "recall = tp/(tp + fn)\n",
    "accuracy = (tn + tp)/ (tn + fn + fp + tp)\n",
    "\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision %.2f' %precision_score(y_train_0, y_hat_train_0))\n",
    "print('Recall %.2f' %recall_score(y_train_0, y_hat_train_0))\n",
    "print('-'*50)\n",
    "print(classification_report(y_train_0, y_hat_train_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision - Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_clf = Perceptron(random_state=1729)\n",
    "bin_clf.fit(x_train, y_train_0)\n",
    "y_scores = bin_clf.decision_function(x_train)\n",
    "sns.histplot(np.sort(y_scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Class imbalance\n",
    "* Suppose threshold talkes the value of -600, then all the samples having score greater than -600  is set to 1 and less than it is set to -1.\n",
    "* On the other hand, if the threshold takes the value of, say, 400. Then, the number of false negatives will increase and recall will reduce to a great extend.\n",
    "\n",
    "Let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_train_0, y_scores, pos_label=1)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(precisions[:-1], recalls[:-1], \"b--\")\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "plt.xlabel('Threshold')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train_0, y_scores)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(fpr, tpr, linewidth=2, label = 'Perceptron')\n",
    "plt.plot([0, 1], [0,1], 'k--', label = 'baseEstimator')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
