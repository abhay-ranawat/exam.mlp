{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with sklearn API\n",
    "\n",
    "Setup:\n",
    "\n",
    "1. Dataset: California housing\n",
    "2. Linear regression API: `LinearRegression`\n",
    "3. Training: `fit`(normal equation) and `cross_validate`(normal equation with cross validation).\n",
    "4. Evaluation: `score`($R^2$ Score) and `cross_val_score` with different scoring parameters.\n",
    "\n",
    "We will study the model diagnosis with `LearningCurve` and learn how to examine the learned model or weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(306)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `ShuffleSplit` cross validation with:\n",
    "* 10 folds (`n_splits`) and\n",
    "* Set aside 20% examples as test examples(`test_size`) in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_split_cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1**: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "\n",
    "print(\"shape of feature matrix: \", features.shape)\n",
    "print(\"shape of label matrix: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(features.shape[0] == labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2**: Data exploration\n",
    "\n",
    "[Done in seperate notebook](California_housing_dataset_exploration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3**: Preprocessing and model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, random_state=42)\n",
    "\n",
    "print(\"# training samples: \", train_features.shape[0])\n",
    "print(\"# training samples: \", test_features.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (train_features.shape[0] == train_labels.shape[0])\n",
    "assert (test_features.shape[0] == test_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pipeline: Preprocessing + Model\n",
    "\n",
    "`Pipeline` object we are going to use have two components:\n",
    "1. `StandardScaler`\n",
    "2. `LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_pipeline = Pipeline([(\"feature_scaling\", StandardScaler( )),\n",
    "                            (\"lin_reg\", LinearRegression())])\n",
    "\n",
    "lin_reg_pipeline.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the model, let's check the learned/estimated weight vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept (w_0):\", lin_reg_pipeline[-1].intercept_) # intercept through intercept_\n",
    "print(\"Weight vector (w_1,....., w_m):\", lin_reg_pipeline[-1].coef_) # rest of the weights through weight_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4**: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `score`\n",
    "\n",
    "* $R^2$ score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model performance in the test set.add\n",
    "test_score = lin_reg_pipeline.score(test_features, test_labels)\n",
    "print(\"Model performance on test set: \", test_score)\n",
    "\n",
    "train_score = lin_reg_pipeline.score(train_features, train_labels)\n",
    "print(\"Model performance on train set: \", train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`r2` score is not high enough $=>$ underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validated score(`cross_val_score`)\n",
    "\n",
    "* Calculates `r2` on different folds through cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_score = cross_val_score(lin_reg_pipeline,\n",
    "                                train_features,\n",
    "                                train_labels,\n",
    "                                scoring = 'neg_mean_squared_error',\n",
    "                                cv = shuffle_split_cv)\n",
    "\n",
    "#This will print 10 different scores, one for each score\n",
    "print(lin_reg_score)\n",
    "\n",
    "# We can take the mean and standard deviation of the score and report it.\n",
    "print(f\"\\nScore of linear regression model on the test set: \\n\"\n",
    "        f\"{lin_reg_score.mean():.3f} +/- {lin_reg_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other available 'scoring` parameters\n",
    "* `explained_variance`\n",
    "* `max_error`\n",
    "* `neg_mean_absolute_error`\n",
    "* `neg_root_mean_squared_error`\n",
    "* `neg_mean_squared_log_error`\n",
    "* `neg_median_absolute_error`\n",
    "* `neg_mean_absolute_percentaage_error`\n",
    "* `r2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "* `cross_validate` gives access to models trained in each fold along with some other statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_cv_results = cross_validate(lin_reg_pipeline,\n",
    "                                    train_features,\n",
    "                                    train_labels,\n",
    "                                    cv = shuffle_split_cv,\n",
    "                                    scoring=\"neg_mean_squared_error\",\n",
    "                                    return_train_score=True,\n",
    "                                    return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiply these scores by -1 to convert them into errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = -1 * lin_reg_cv_results['train_score']\n",
    "test_error = -1 * lin_reg_cv_results['test_score']\n",
    "\n",
    "print(f\"Mean squareed error of linear regression model on the train set:\\n\"\n",
    "        f\"{train_error.mean():.3f} +/- {train_error.std():.3f}\")\n",
    "\n",
    "print(f\"Mean squareed error of linear regression model on the test set:\\n\"\n",
    "        f\"{test_error.mean():.3f} +/- {test_error.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The training and test errors are high, which is an indication of underfitting, which we will confirm by plotting the learnig curve.\n",
    "* Test error has high variability across different folds compared to the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of training set size on error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for visualisation\n",
    "\n",
    "def plot_learning_curve(train_sizes, train_scores, test_scores):\n",
    "\n",
    "    train_scores_mean = np.mean(-train_scores, axis=1)\n",
    "    train_scores_std = np.std(-train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(-test_scores, axis=1)\n",
    "    test_scores_std = np.std(-test_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(\n",
    "            train_sizes,\n",
    "            train_scores_mean - train_scores_std,\n",
    "            train_scores_mean + train_scores_std,\n",
    "            alpha = 0.1,\n",
    "            color = \"r\",)\n",
    "\n",
    "    plt.fill_between(\n",
    "            train_sizes,\n",
    "            test_scores_mean - test_scores_std,\n",
    "            test_scores_mean + test_scores_std,\n",
    "            alpha = 0.1,\n",
    "            color = \"g\",)\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scores calculated by `learning_curve` API, we plot the error and its standard deviation for different number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sizes, train_scores, test_scores, fit_times, score_times) = learning_curve(lin_reg_pipeline,\n",
    "                                                                                    train_features,\n",
    "                                                                                    train_labels,\n",
    "                                                                                    cv=shuffle_split_cv,\n",
    "                                                                                    scoring='neg_mean_squared_error',\n",
    "                                                                                    n_jobs=-1,\n",
    "                                                                                    return_times=True,\n",
    "                                                                                    train_sizes=np.linspace(0.2, 1.0, 10))\n",
    "\n",
    "plot_learning_curve(train_sizes, train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations:\n",
    "\n",
    "* Both curves have reached a plateau\" They are close and fairly high.\n",
    "* Few instances in the training set mean the model can fit them perfectly. But as more instances are added to the training set, it becomes impossible for the model to fit the training data perfectly.\n",
    "* When the model is trained on very few instances, it is not able to generalize properly, hence the validation error is high.\n",
    "\n",
    "These learning curves are typical of underfitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Examination\n",
    "\n",
    "Let's examine the weight vectors and how much variability exists between them across different cross-validated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = train_features.columns\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [est[-1].coef_ for est in lin_reg_cv_results[\"estimator\"]]\n",
    "weights_df = pd.DataFrame(coefs, columns=feature_names)\n",
    "\n",
    "color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\n",
    "weights_df.plot.box(color=color, vert=False)\n",
    "_ = plt.title(\"Linear regression coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much variability in weights learned by different models. It can also be seen from the std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_index = np.argmin(test_error)\n",
    "selected_model = lin_reg_cv_results['estimator'][best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept (w_0):\", selected_model['lin_reg'].intercept_) # intercept through intercept_\n",
    "print(\"Weight vector (w_1,....., w_m):\", selected_model['lin_reg'].coef_) # rest of the weights through weight_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions = cross_val_predict(lin_reg_pipeline, train_features, train_labels)\n",
    "\n",
    "mse_cv = mean_squared_error(train_labels, cv_predictions)\n",
    "\n",
    "plt.scatter(train_labels, cv_predictions, color='blue')\n",
    "plt.plot(train_labels, train_labels, 'r-')\n",
    "plt.title(f\"Mean squared error = {mse_cv:.2f}\", size=24)\n",
    "plt.xlabel(\"Actual Median House Value\", size=15)\n",
    "plt.ylabel(\"Predicted Median House Value\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we should perform error analysis and check where the predictions are going wrong. We can revisit feature construction, preprocessing and model stages and make the necessary course correction to get better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_cv = selected_model.predict(test_features)\n",
    "test_predictions_cv[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain predictions using initial model that we built without cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lin_reg_pipeline.predict(test_features)\n",
    "test_predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Report model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cv = selected_model.score(test_features, test_labels)\n",
    "score = lin_reg_pipeline.score(test_features, test_labels)\n",
    "\n",
    "print(\"R2 score for the best model obtained via cross validation: \", score_cv)\n",
    "print(\"R2 score for the model without cross validation: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validated models have slightly better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
