{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineaar regression with iterative optimisation(`SGDRegressor`)\n",
    "\n",
    "It gives conrtrol over optimisation through a number of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(306)\n",
    "\n",
    "shuffle_split_cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "com_train_features, test_features, com_train_labels, test_labels = train_test_split(features, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, dev_features, train_labels, dev_labels = train_test_split(com_train_features, com_train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline `SGDRegressor`\n",
    "\n",
    "* STEP 1: Instantiate baseline `SGDRegressor` with default parameters.\n",
    "* STEP 2: Train the model with training feature matrix and labels.\n",
    "* STEP 3: Obtain the score on the train and dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(random_state=42)\n",
    "sgd.fit(train_features, train_labels)\n",
    "\n",
    "train_mae = mean_absolute_error(train_labels, sgd.predict(train_features))\n",
    "dev_mae = mean_absolute_error(dev_labels, sgd.predict(dev_features))\n",
    "\n",
    "print(\"Mean Absolute Error on Training set: \", train_mae)\n",
    "print(\"Mean Absolute Error on development set: \", dev_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors are too large!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a feature scaling step\n",
    "\n",
    "SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipeline = Pipeline([(\"feature_scaling\", StandardScaler()),\n",
    "                        (\"sgd\", SGDRegressor())])\n",
    "\n",
    "sgd_pipeline.fit(train_features, train_labels)\n",
    "\n",
    "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
    "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
    "\n",
    "print(\"Mean Absolute Error on Training set: \", train_mae)\n",
    "print(\"Mean Absolute Error on development set: \", dev_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-wise training of SGDRegressor\n",
    "\n",
    "* STEP 1: Instantiate `SGDRegressor` with `warm_start=True` and `tol=-np.infty`\n",
    "* STEP 2: Train SGD step by step and recod regression loss in each step.\n",
    "* STEP 3: Plot learning curve and see if there are any issues in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta0 = 1e-2\n",
    "sgd_pipeline = Pipeline([(\"feature_scaling\", StandardScaler()),\n",
    "                        (\"sgd\", SGDRegressor(max_iter=1, tol=-np.infty,\n",
    "                                                warm_start=True,\n",
    "                                                random_state=42))])\n",
    "\n",
    "loss = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    sgd_pipeline.fit(train_features, train_labels)\n",
    "    loss.append(mean_squared_error(train_labels, sgd_pipeline.predict(train_features)))\n",
    "\n",
    "plt.plot(np.arange(len(loss)), loss, 'b-')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(f'Learning curve: eta0={eta0: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss reduced intially and then increased. This could be due to large training rates. We will reduce the rate by a factor of 10 and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta0 = 1e-3\n",
    "sgd_pipeline = Pipeline([(\"feature_scaling\", StandardScaler()),\n",
    "                        (\"sgd\", SGDRegressor(max_iter=1, tol=-np.infty,\n",
    "                                                warm_start=True, eta0=eta0,\n",
    "                                                random_state=42))])\n",
    "\n",
    "loss = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    sgd_pipeline.fit(train_features, train_labels)\n",
    "    loss.append(mean_squared_error(train_labels, sgd_pipeline.predict(train_features)))\n",
    "\n",
    "plt.plot(np.arange(len(loss)), loss, 'b-')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(f'Learning curve: eta0={eta0: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ideal learning curve where the train loss reduce monotonically as the training progresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# iteration before reaching convergence criteria: ', \n",
    "        sgd_pipeline[-1].n_iter_)\n",
    "\n",
    "print (\"#Weight updated:\", sgd_pipeline[-1].t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
    "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
    "\n",
    "print(\"Mean Absolute Error on Training set: \", train_mae)\n",
    "print(\"Mean Absolute Error on development set: \", dev_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing learning rate through validation curves\n",
    "\n",
    "* STEP 1: Provide the list of values to be tried for a hyper=parameter.\n",
    "* STEP 2: Instantiate an object of `validation_curve` with estimator, training features and label. Set `scoring` parameter to relevant score.\n",
    "* STEP 3: Convert sccores to error.\n",
    "* STEP 4: Fix the hyper parameter value where the test error is the least. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "eta0 = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "train_scores, test_scores = validation_curve(\n",
    "                                            sgd_pipeline, com_train_features, com_train_labels, param_name=\"sgd__eta0\",\n",
    "                                            param_range=eta0, cv = shuffle_split_cv, scoring=\"neg_mean_squared_error\",\n",
    "                                            n_jobs=2)\n",
    "\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eta0, train_errors.mean(axis=1), 'b-x', label='Training error')\n",
    "plt.plot(eta0, test_errors.mean(axis=1), 'r-x', label='Test error')\n",
    "plt.legend()\n",
    "plt.xlabel(\"eta0\")\n",
    "plt.ylabel(\"Mean absolute error ($k$)\")\n",
    "_ = plt.title(\"Validation curve for SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SGDRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipeline = Pipeline([(\"feature_scaling\", StandardScaler()),\n",
    "                        (\"sgd\", SGDRegressor(max_iter=500,\n",
    "                                            early_stopping=True,\n",
    "                                            eta0 = 1e-3,\n",
    "                                            tol=1e-3,\n",
    "                                            validation_fraction=0.2,\n",
    "                                            n_iter_no_change=5,\n",
    "                                            average=10,\n",
    "                                            random_state=42))])\n",
    "\n",
    "sgd_pipeline.fit(train_features, train_labels)\n",
    "\n",
    "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
    "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
    "\n",
    "print(\"Mean Absolute Error on Training set: \", train_mae)\n",
    "print(\"Mean Absolute Error on development set: \", dev_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# iteration before reaching convergence criteria: ', \n",
    "        sgd_pipeline[-1].n_iter_)\n",
    "\n",
    "print (\"#Weight updated:\", sgd_pipeline[-1].t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate changed to constant from inverse scaling(default)\n",
    "sgd_pipeline = Pipeline([(\"feature_scaling\", StandardScaler()),\n",
    "                        (\"sgd\", SGDRegressor(max_iter=500,\n",
    "                                            early_stopping=True,\n",
    "                                            eta0 = 1e-3,\n",
    "                                            tol=1e-3,\n",
    "                                            learning_rate='constant',\n",
    "                                            validation_fraction=0.2,\n",
    "                                            n_iter_no_change=5,\n",
    "                                            average=10,\n",
    "                                            random_state=42))])\n",
    "\n",
    "sgd_pipeline.fit(train_features, train_labels)\n",
    "\n",
    "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
    "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
    "\n",
    "print(\"Mean Absolute Error on Training set: \", train_mae)\n",
    "print(\"Mean Absolute Error on development set: \", dev_mae)\n",
    "\n",
    "print('\\n# iteration before reaching convergence criteria: ', \n",
    "        sgd_pipeline[-1].n_iter_)\n",
    "\n",
    "print (\"#Weight updated:\", sgd_pipeline[-1].t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate changed to adaptive from inverse scaling(default)\n",
    "sgd_pipeline = Pipeline([(\"feature_scaling\", StandardScaler()),\n",
    "                        (\"sgd\", SGDRegressor(max_iter=500,\n",
    "                                            early_stopping=True,\n",
    "                                            eta0 = 1e-3,\n",
    "                                            tol=1e-3,\n",
    "                                            learning_rate='adaptive',\n",
    "                                            validation_fraction=0.2,\n",
    "                                            n_iter_no_change=5,\n",
    "                                            average=10,\n",
    "                                            random_state=42))])\n",
    "\n",
    "sgd_pipeline.fit(train_features, train_labels)\n",
    "\n",
    "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
    "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
    "\n",
    "print(\"Mean Absolute Error on Training set: \", train_mae)\n",
    "print(\"Mean Absolute Error on development set: \", dev_mae)\n",
    "\n",
    "print('\\n# iteration before reaching convergence criteria: ', \n",
    "        sgd_pipeline[-1].n_iter_)\n",
    "\n",
    "print (\"#Weight updated:\", sgd_pipeline[-1].t_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting `max_iters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = np.ceil(1e6/com_train_features.shape[0])\n",
    "max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipeline = Pipeline([(\"feature_scaling\", StandardScaler()),\n",
    "                        (\"sgd\", SGDRegressor(max_iter=max_iter,\n",
    "                                            early_stopping=True,\n",
    "                                            eta0 = 1e-3,\n",
    "                                            tol=1e-3,\n",
    "                                            learning_rate='adaptive',\n",
    "                                            validation_fraction=0.2,\n",
    "                                            n_iter_no_change=5,\n",
    "                                            average=10,\n",
    "                                            random_state=42))])\n",
    "\n",
    "sgd_pipeline.fit(train_features, train_labels)\n",
    "\n",
    "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
    "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
    "\n",
    "print(\"Mean Absolute Error on Training set: \", train_mae)\n",
    "print(\"Mean Absolute Error on development set: \", dev_mae)\n",
    "\n",
    "print('\\n# iteration before reaching convergence criteria: ', \n",
    "        sgd_pipeline[-1].n_iter_)\n",
    "\n",
    "print (\"#Weight updated:\", sgd_pipeline[-1].t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
