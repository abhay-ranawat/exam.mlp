{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Techniques\n",
    "* Data cleaning\n",
    "    * Data Imputation\n",
    "    * Feature Scaling\n",
    "* Feature transformations\n",
    "    * Polynomial Features\n",
    "    * Discretization\n",
    "    * Handling categorical variables\n",
    "    * Custom Transformers\n",
    "    * Composite Transformers\n",
    "        * Apply composite feature to diverse features\n",
    "        * TargetTransformRegressor\n",
    "    * Feature Selection\n",
    "        * Filter based methods\n",
    "        * Wrapper based Methods\n",
    "    * Feature extraction\n",
    "        * PCA\n",
    "\n",
    "These transformations are applied in a specific order and the order can be specified via Pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DictVectorizer\n",
    " `DictVectorizer` converts a list of dictionary objects to feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'age' : 4, 'height' : 96.0},\n",
    "        {'age' : 1, 'height' : 73.9},\n",
    "        {'age' : 3, 'height' : 88.9},    \n",
    "        {'age' : 2, 'height' : 81.6}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "data_transformed = dv.fit_transform(data)\n",
    "data_transformed, data_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Imputation\n",
    "\n",
    "* Many Ml algos need full feature matrix\n",
    "* Data Imputation identifies missing values in each features and replaces them with a strategy such as \n",
    "    * Mean/Median/mode.\n",
    "    * user specified constant value.\n",
    "    \n",
    "sklearn library provides `sklearn.impute.SimpleImputer` class for this purpose\n",
    "\n",
    "`add_indicator` is a boolean parameter when set to `True` returns missing value indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get some real world data!!\n",
    "\n",
    "cols = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg' , 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n",
    "heart_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None, names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 1:** Check if dataset contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data.info() # for numerical values. For str, check unique values etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(heart_data.isnull().sum()) #Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in ca: \", heart_data.ca.unique())\n",
    "print(\"Unique values in thal: \", heart_data.thal.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`?` missing values. Let's count the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Missing values in ca:\", heart_data.loc[heart_data.ca == '?', 'ca'].count())\n",
    "print(\"# Missing values in thal:\", heart_data.loc[heart_data.thal == '?', 'thal'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 2:** Replace '?' with `nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:** Fill the missing `sklearn` missing value imputation utilities.\n",
    "\n",
    "Here we use `SimpleImputer` with `mean` strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(heart_data)\n",
    "heart_data_imputed = imputer.transform(heart_data)\n",
    "print(heart_data_imputed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`add_indicator = True:` Adds additional column for each column containing missing values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "imputer = imputer.fit(heart_data)\n",
    "heart_data_imputed_with_indicator = imputer.transform(heart_data)\n",
    "print(heart_data_imputed_with_indicator.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling\n",
    "Feature scaling transforms feature values such that all the features are on the same scale. \n",
    "* Enables faster convergence in iterative optimization\n",
    "* Algos which uses euclidean distance b/n features also suffer\n",
    "* Tree based ML algos are not affected by feature scaling - So feature scaling not required\n",
    "\n",
    "Feature scaaling can be performed with the following methods:\n",
    "* Standardization\n",
    "* Normalisation\n",
    "* MaxAbsScaler\n",
    "\n",
    "Data Set:  [Abalone dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\n",
    "abalone_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', header = None, names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 1:** Examine the dataset\n",
    "Feature scaling performed only on numerical attributes. Let's check which are numerical attributes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 1a:**  Convert non-numerical attributes to numerical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data.Sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign numerical values to sex.\n",
    "abalone_data = abalone_data.replace({\"Sex\": {\"M\":1, \"F\":2, \"I\":3}})\n",
    "abalone_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 2:** Seperate features from labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = abalone_data.pop('Rings')\n",
    "print(\"The Dataframe object after deleting label\")\n",
    "abalone_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STEP 3:** Examine feature scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation of feature distributions\n",
    "\n",
    "* Histograms\n",
    "* Kernel Density Estimation(KDE) Plot\n",
    "* Box\n",
    "* Violin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seperate Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=2)\n",
    "fig.suptitle(\"Individual histograms\")\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "for i in range(len(abalone_data.columns)):\n",
    "    fig.add_subplot(4,2,i+1)\n",
    "    fig.tight_layout()\n",
    "    plt.hist(abalone_data[abalone_data.columns[i]])\n",
    "    plt.title(abalone_data.columns[i])\n",
    "    plt.xlabel('Range')\n",
    "    plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histograms together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms collated\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "for feature in abalone_data.columns:\n",
    "    plt.hist(abalone_data[feature], alpha=0.5, label=feature)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Distribution of features across samples')\n",
    "plt.xlabel('Range')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KDE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = abalone_data.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = abalone_data.plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'whitegrid')\n",
    "sns.violinplot(data=abalone_data, orient=\"h\", scale=\"width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STEP 4:** Scaling\n",
    "\n",
    "* Normalisation\n",
    "    * `MaxAbsoluteScaler` transaforms features into range [-1,1]\n",
    "        * x' = x/MaxAbsoluteValue \n",
    "        * MaxAbsoluteValue = max(x.max, |x.min|)\n",
    "    * `MinMaxScaler` transforms feature in range [0,1]\n",
    "        * x_new = (x_old - x_min)/(x_max - x_min)\n",
    "\n",
    "* Standardisation\n",
    "    * `StandardScaler`\n",
    "    * X_new = (X_old - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([4, 2, 5, -2, -100]).reshape(-1, 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "mas = MaxAbsScaler()\n",
    "x_new = mas.fit_transform(x)\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X = abalone_data\n",
    "mm = MinMaxScaler()\n",
    "X_normalised = mm.fit_transform(X)\n",
    "X_normalised[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalised.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalised.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 6)\n",
    "cols = abalone_data.columns\n",
    "df = pd.DataFrame(X_normalised, columns=cols)\n",
    "\n",
    "for feature in df.columns:\n",
    "    plt.hist(df[feature], alpha=0.5, label=feature)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Distribution of features across samples')\n",
    "plt.xlabel('Range')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot of transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = df.plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Violin Plot of tranformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'whitegrid')\n",
    "sns.violinplot(data= df, orient=\"h\", scale=\"width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KDE Plot of transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_standardised = ss.fit_transform(X)\n",
    "X_standardised.mean(axis=0), X_standardised.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of standardised features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 6)\n",
    "cols = abalone_data.columns\n",
    "df = pd.DataFrame(X_standardised, columns=cols)\n",
    "\n",
    "for feature in df.columns:\n",
    "    plt.hist(df[feature], alpha=0.5, label=feature)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Distribution of features across samples')\n",
    "plt.xlabel('Range')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot of standardised features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = df.plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Violin Plot of standardised features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'whitegrid')\n",
    "sns.violinplot(data= df, orient=\"h\", scale=\"width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KDE Plot of standardised features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `add_dummy_feature`\n",
    "Augments dataset with a column vector of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "        [[7,1],\n",
    "        [1, 8],\n",
    "        [2, 0],\n",
    "        [9, 6]])\n",
    "\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "x_new = add_dummy_feature(x)\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom transformers\n",
    "\n",
    "Enables conversion of an existing Python function into a transformer to assist in data cleaning or preprocessing\n",
    "\n",
    "Useful when:\n",
    "1. Dataset consists of heterogenous datatypes\n",
    "2. When different columns require different transformations\n",
    "3. We need stateless transformations such as taking the log of frequencies, custom scaling etc \n",
    "\n",
    "\n",
    "Dataset: [Wine Quality dataset from UCI ML repository](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=\";\") \n",
    "wine_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `np.log1p` which returns natural logarithm of (1 + the feature Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "wine_data_transformed = transformer.transform(np.array(wine_data))\n",
    "pd.DataFrame(wine_data_transformed, columns=wine_data.columns).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Polynomial Features\n",
    "\n",
    "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree\n",
    "\n",
    "* `sklearn.preprocessing.PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "wine_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=\";\") \n",
    "wine_data_copy = wine_data.copy()\n",
    "wine_data = wine_data.drop(['quality'], axis=1)\n",
    "print ('Number of features before transformation = ', wine_data.shape)\n",
    "\n",
    "#Let's apply a polynomial transform of the order 2 to wine_data\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly_wine_data = poly.fit_transform(wine_data)\n",
    "print(\"Number of features after transformation= \", poly_wine_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discretization\n",
    "\n",
    "Discretization/quantization/binning provides a way to partition continuos features into discrete values.\n",
    "\n",
    "* Certain dataasets with continuous features may benefit from discretisation, because it transforms continous attibutes to nominal attributes.\n",
    "* One-hot encoded discretised freatues can make a model more expressive, while maintaing interpretability.\n",
    "* For instance, pre-processing witha discretizer can introduce non-linearity to linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KBinsDiscretizer discretizes features into k bins\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "wine_data = wine_data_copy.copy()\n",
    "\n",
    "#Traansform the dataset with KBinsDiscretizer\n",
    "enc = KBinsDiscretizer(n_bins=10, encode=\"onehot\")\n",
    "X = np.array(wine_data['chlorides']). reshape(-1,1)\n",
    "X_binned = enc.fit_transform(X)\n",
    "X_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binned.toarray()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling Categorical Features\n",
    "\n",
    "The following methods can be used to convert the categorical features into numeric features\n",
    "\n",
    "1. Ordinal encoding\n",
    "2. One-hot encoding\n",
    "3. Label encoding\n",
    "4. Using dummy variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal encoding\n",
    "Assigns unique numerical value to each unique non-numerical feature. But it would introduce numerical relationship which may or may not exist between the non-numerical entities\n",
    "* Implemented using `OrdinalEncoder` class from `sklearn.preprocessing` module\n",
    "\n",
    "### One-hot encoding\n",
    "This approach consists of creating an additional feature for each label present in the categorical feature and putting 1 or 0 for these new features depending on the categorical feature's value.\n",
    "* Implemented using `OneHotEncoder` class from `sklearn.preprocessing` module\n",
    "\n",
    "Dataset: [Iris]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cols = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "iris_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None, names=cols)\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `label` is a categorical attribite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert them to One-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "print(\" Shape of y before encoding\", iris_data.label.shape)\n",
    "\n",
    "'''\n",
    "Passing 1d arraysas data to onehotencoder is deprecated.\n",
    "Hence reshape to (-1,1) to have two dimensions\n",
    "Input of OneHotEncoder fit_transform must not be one dimensional array\n",
    "'''\n",
    "iris_labels = onehotencoder.fit_transform(iris_data.label.values.reshape(-1, 1))\n",
    "\n",
    "print('Shape of y after encoding = ',iris_labels.shape)\n",
    "\n",
    "print(\"First five labels:\")\n",
    "print(iris_labels.toarray()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us observe the difference between one hot encoding and ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "iris_labels = np.array(iris_data['label'])\n",
    "\n",
    "iris_labels_transformed = enc.fit_transform(iris_labels.reshape(-1, 1))\n",
    "print(\"Unique labels: \", np.unique(iris_labels_transformed))\n",
    "\n",
    "print(\"\\nFirst 5 labels:\")\n",
    "print(iris_labels_transformed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEncoder\n",
    "\n",
    "Another option is to use `LabelEncoder` for transforming categorical features into integer codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "iris_labels = np.array(iris_data['label'])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_integer = enc.fit_transform(iris_labels)\n",
    "label_integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultilabelBinarizer\n",
    "\n",
    "Encodes categorical featurees with value between $0$ and $k-1$, where $k$ is number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_genres = [{'action', 'comedy'},\n",
    "                {'comedy'},\n",
    "                {'action', 'thriller'},\n",
    "                {'science-fiction', 'action', 'thriller'} ]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform(movie_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_dummies to create a one_hot encoding for each unique categorical value in the 'class column\n",
    "# convert categorical classs variable to one hot encoding:\n",
    "iris_data_onehot = pd.get_dummies(iris_data, columns=['label'], prefix=['one_hot'])\n",
    "iris_data_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Composite Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColumnTransformer\n",
    "\n",
    "It applies a set of transdformers to columns of an array or `pandas.DataFrame`, concactenates the transformed outputs from different transformers into a single matrix.\n",
    "* It is useful for transforming heterogenous data by applying different transformers to seperate subset of features\n",
    "* It combines different feature selcetion mechanisms and transformations into a single transformer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    [20.0, 'male',],\n",
    "    [11.2, 'female',],\n",
    "    [15.6, 'female',],\n",
    "    [13.0, 'male',],\n",
    "    [18.6, 'male',],\n",
    "    [16.4, 'female',],\n",
    "]\n",
    "\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler, OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer([('scaler', MaxAbsScaler(), [0]),\n",
    "                        ('pass', 'passthrough', [0]),\n",
    "                        ('encoder', OneHotEncoder(), [1])])\n",
    "ct.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformedTargetRegressor\n",
    "\n",
    "Transforms the target variable y before fitting a regression model.\n",
    "* The predicted values are mapped back to the original space via an inverse transform.\n",
    "* It takes regressor and transformer to be applied to the taret as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn. model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X, y = X[:2000,:], y[:2000] #select a subset of data\n",
    "\n",
    "transformer = MaxAbsScaler()\n",
    "\n",
    "regressor = LinearRegression()\n",
    "\n",
    "regr = TransformedTargetRegressor(regressor=regressor, transformer=transformer)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "print(\"R2 score of transformed label regression: {0:.2f}\".format(regr.score(X_test, y_test)))\n",
    "\n",
    "raw_target_regr = LinearRegression().fit(X_train, y_train)\n",
    "print(\"R2 score of raw label regression: {0:.2f}\".format(raw_target_regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Selection\n",
    "\n",
    "`sklearn.feature_selction` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VarianceThreshold\n",
    "\n",
    "This transformer helps to keep only high variance features by providing a certain threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'age': 4, 'height': 96.0},\n",
    "        {'age': 1, 'height': 73.9},\n",
    "        {'age': 3, 'height': 88.9},\n",
    "        {'age': 2, 'height': 81.6}]\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "data_transformed = dv.fit_transform(data)\n",
    "np.var(data_transformed, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "vt = VarianceThreshold(threshold=9)\n",
    "data_new = vt.fit_transform(data_transformed)\n",
    "data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectKBest\n",
    "It selects $k$ highest scoring features based on a function and removes the rest of the features.\n",
    "\n",
    "Dataset: [California Housing]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "#Download data\n",
    "X_cal, y_cal = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "#Select a subset of data\n",
    "\n",
    "X, y = X_cal[:2000,:], y_cal[:2000]\n",
    "\n",
    "print(f'Shape of feature matrix before feature selection:{X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select 3 most important features, we can use only `mutual_info_regression` or `f_regression` functions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb = SelectKBest(mutual_info_regression, k = 3)\n",
    "X_new = skb.fit_transform(X, y)\n",
    "\n",
    "print(f'Shape of feature matrix after feature selection:{X_new.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectPercentile\n",
    "\n",
    "This is very similar to `SelectKBest`, the only difference being that it select upto the top `percentile`of all features abd drops the rezt of the features. Uses a a Scoring function like `SelectKBest` as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. feature_selection import SelectPercentile\n",
    "sp = SelectPercentile(mutual_info_regression, percentile = 30)\n",
    "X_new = sp.fit_transform(X, y)\n",
    "print(f'Shape of feature matrix after feature selection:{X_new.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GenericUnivariateSelect\n",
    "\n",
    "It applies univariate feature selection with a strategy, which is passed to the API via `mode` parameter. `mode` can take the following values: `percentile`, `k_best`, `fpr`(false positive ratio), `fdr`(false discovery ratio), `fwe`(family wise error rate). To obtain the same result as `SelectKBest`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "gus = GenericUnivariateSelect(mutual_info_regression, mode = 'k_best', param = 3)\n",
    "X_new = gus.fit_transform(X, y)\n",
    "print(f'Shape of feature matrix before feature selection:{X.shape}')\n",
    "print(f'Shape of feature matrix after feature selection:{X_new.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE(Recursive Feature Elimination)\n",
    "**STEP 1** : Fits a model.\n",
    "\n",
    "**STEP 2** : Ranks the features, afterwards it removes one or more features dependent on `step` parameter.\n",
    "\n",
    "**STEP 3** : Repeat till we reach the desired number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "selector = RFE(estimator, n_features_to_select=3, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "#support_ attribute is a boolean array\n",
    "#marking which features are selected\n",
    "print(selector.support_)\n",
    "\n",
    "#rank of each feature\n",
    "#if it's value is '1', Then it is selected\n",
    "#features with rank 2 and onwards are ranked least.\n",
    "print(f'Rank of each feature is: {selector.ranking_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = selector.transform(X)\n",
    "print(f'Shape of feature matrix after feature selection:{X_new.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectFromModel\n",
    "\n",
    "Select desired number of important features above certain threshold of feature importance as obtained from the trained estimator.\n",
    "\n",
    "* The feature importance is obtained via `coef_`, `feature_importances` or an `importance_getter` callable from the trained estimator\n",
    "* The feature importance threshold can be specified either numerically or through string argument based on built in heuristic such as `mean`, `median` and float multiples of these like `0.1*mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "estimator = LinearRegression()\n",
    "estimator.fit(X, y)\n",
    "\n",
    "print(f'Coefficients of features: {estimator.coef_}')\n",
    "print(f'Indices of top {3} features: {np.argsort(estimator.coef_)[-3:]}')\n",
    "\n",
    "t = np.argsort(np.abs(estimator.coef_))[-3:]\n",
    "model = SelectFromModel(estimator, max_features=3, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(f'Shape of feature matrix after feature selection:{X_new.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SequentialFeatureSelector\n",
    "\n",
    "It performs feature selection by selecting or deselecting features one by one in a greedy manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "sfs = SequentialFeatureSelector(estimator, n_features_to_select=3)\n",
    "sfs.fit_transform(X, y)\n",
    "print(sfs.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features corresponding to `True` in the output `sfs.get_support()` are seleccted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator = LinearRegression()\n",
    "sfs = SequentialFeatureSelector(estimator, n_features_to_select=3, direction='backward')\n",
    "sfs.fit_transform(X, y)\n",
    "print(sfs.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator = LinearRegression()\n",
    "sfs = SequentialFeatureSelector(estimator, n_features_to_select=3, direction='forward')\n",
    "sfs.fit_transform(X, y)\n",
    "print(sfs.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca  = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Chaining Transformers\n",
    "\n",
    "The preprocessing transformations are applied one after the another on the input feature matrix. It is important to apply exactly the same transformation on training, evaluation and testing sets in the same order. \n",
    "\n",
    "The `sklearn.pipeline` module provides utilities to build a compsite estimator, as a chain of transformers and estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Sequencially apply a list if transformers and estimators.\n",
    "* Intermediate steps of the pipeline must be 'transformers'. i.e. They must implement `fit` and `transform` methods.\n",
    "* Final estimator only needs to implement `fit`\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together whilesetting different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "A Pipeline can be created with Pipeline(). It takes a list of `('estimatorName', estimator(..))` tuples. The pipeline object exposes the interrface of the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "estimators = [\n",
    "    ('simpleImputer', SimpleImputer()),\n",
    "    ('standardscaler', StandardScaler())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps=estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same pipeline can als be created via `make_pipeline()` helper function, which doesn't take names of the steps and assigns them generic names based on their steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(SimpleImputer(), \n",
    "                     StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Individual steps in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [\n",
    "            ('simpleImputer', SimpleImputer()),\n",
    "            ('pca', PCA()),\n",
    "            ('regressor', LinearRegression())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print number of steps in this pipeline\n",
    "print(len(pipe.steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at each step\n",
    "print(pipe.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing parameters of a step\n",
    "\n",
    "pipe.set_params(pca__n_components = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with pipeline\n",
    "\n",
    "By using naming convention of nested parameters, grid search can be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(imputer = ['passthrough',\n",
    "                             SimpleImputer(),\n",
    "                             KNNImputer()],\n",
    "                    clf = [SVC(), LogisticRegression()],\n",
    "                    clf__C = [0.1, 10, 100])\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `c` is an inverse of regularization, lower its value stronger the regularisation is.\n",
    "* In the example above `clf__C provides a set of values for grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Transformers\n",
    "\n",
    "Transforming data is a computationally expensive step.\n",
    "\n",
    "* For grid search, transformers need not be applied for every parameter configuration. they can be applied once, and the transformed data can be reused.\n",
    "\n",
    "This can be achieved by setting `memory` parameter of a `pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "tempDirPath = tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [ \n",
    "                ('simpleImputer', SimpleImputer()),\n",
    "                ('pca', PCA(2)),\n",
    "                ('regressor', LinearRegression())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps=estimators, memory=tempDirPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnion\n",
    "\n",
    "Concatenates results of multiple traansformer objects.\n",
    "\n",
    "* Applies a list of transformer objects in parallel, and their outputs are concatenated side by side into a largematrix.\n",
    "\n",
    "`FeatureUnion` and `Pipeline` can be used to create complex transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualising Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_pipeline = Pipeline([('selector', ColumnTransformer([('select_first_4',\n",
    "                                                            'passthrough',\n",
    "                                                            slice(0,4))])),\n",
    "                            ('imputer', SimpleImputer(strategy='median')),\n",
    "                            ('std_scaler', StandardScaler()),\n",
    "                            ])\n",
    "\n",
    "cat_pipeline = ColumnTransformer([('label_binarizer', LabelBinarizer(), [4]),\n",
    "                                    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=\n",
    "                            [(\"num_pipeline\", num_pipeline),\n",
    "                                (\"cat_pipeline\", cat_pipeline),\n",
    "                            ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "#displays HTML representation in a jupyter context\n",
    "full_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Handling impalanced data\n",
    "\n",
    "There are two main approached to handle imbalanced data:\n",
    "* Undersampling\n",
    "* Oversampling\n",
    "\n",
    "Dataset: [Wine Quality dataset from UCI ML repository](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=\";\")\n",
    "\n",
    "wine_data['quality'].hist(bins=50)\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class count\n",
    "class_count_3, class_count_4, class_count_5, class_count_6, class_count_7, class_count_8 = wine_data['quality'].value_counts()\n",
    "\n",
    "#seperate class\n",
    "\n",
    "class_3 = wine_data[wine_data['quality'] == 3]\n",
    "class_4 = wine_data[wine_data['quality'] == 4]\n",
    "class_5 = wine_data[wine_data['quality'] == 5]\n",
    "class_6 = wine_data[wine_data['quality'] == 6]\n",
    "class_7 = wine_data[wine_data['quality'] == 7]\n",
    "class_8 = wine_data[wine_data['quality'] == 8]\n",
    "\n",
    "# Print the shape of the class\n",
    "print('class 3:', class_3.shape)\n",
    "print('class 4:', class_4.shape)\n",
    "print('class 5:', class_5.shape)\n",
    "print('class 6:', class_6.shape)\n",
    "print('class 7:', class_7.shape)\n",
    "print('class 8:', class_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "X = wine_data.drop(['quality'], axis=1)\n",
    "y = wine_data['quality']\n",
    "\n",
    "undersample = RandomUnderSampler(random_state = 0)\n",
    "X_rus, y_rus = undersample.fit_resample(X, y)\n",
    "\n",
    "print('Original dataset shape: ', Counter(y))\n",
    "print('Original dataset shape: ', Counter(y_rus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "\n",
    "print('Original dataset shape: ', Counter(y))\n",
    "print('Resample data shape: ', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_ros.shape[0] - X.shape[0], 'New random points generated with RandomOverSampler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling using SMOTE\n",
    "\n",
    "SMOTE(Synthetic Minor Oversampling Technique) is a popular technique for oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_sm, y_sm = oversample.fit_resample(X, y)\n",
    "counter = Counter(y_sm)\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of SMOTE**\n",
    "\n",
    "* Borderline SMOTE\n",
    "* Borderline-SMOTE SVM\n",
    "* Adaptive Synthetic Sampling(ADASYN)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
